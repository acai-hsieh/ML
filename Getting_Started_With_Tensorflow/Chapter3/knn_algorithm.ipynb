{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 3 \n",
    "# Starting with Machine Learning 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contents #\n",
    "***\n",
    "- [ ] Linear regression\n",
    "- [ ] The MNIST dataset\n",
    "- [x] Classifiers\n",
    "- [x] The nearest neighbor algorithm\n",
    "- [ ] Data clustering\n",
    "- [ ] The k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifiers\n",
    "***\n",
    "* Assigns each new input datum (instance) to one of the possible categories (classes). \n",
    "* Binary classification / Multiclass classification.\n",
    "* Supervised learning category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The basic steps to follow to resolve a supervised classification problem are as follows:**\n",
    "1. Build the training examples in order to represent the actual context and application on which to accomplish the classification. \n",
    "2. Choose the classifier and the corresponding algorithm implementation.\n",
    "2. Train the algorithm on the training set and set any control parameters through validation. \n",
    "4. Evaluate the accuracy and performance of the classifier by applying a set of new instances (test set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The nearest neighbor algorithm\n",
    "***\n",
    "The K-nearest neighbor (KNN) is a supervised learning algorithm for both classification or regression. It is a system that assigns the class of the sample tested according to its distance from the objects stored in the memory.\n",
    "\n",
    "<center><img src=\"imgs\\knn_mean_of_intensity.gif\" width=\"50%\" height=\"50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eulidean Distance\n",
    "***\n",
    "\n",
    "The distance, d, is defined as the Euclidean distance between two points:\n",
    "<center><img src=\"imgs\\knn_equation.png\" width=\"40%\" height=\"40%\"></center>\n",
    "\n",
    "* n is the dimension of the space.\n",
    "* The advantage is the ability to classify objects whose classes are not linearly separable.\n",
    "* Stable classifier.\n",
    "* Small perturbations of the training data do not significantly affect the results obtained. \n",
    "* For every new classification, it should be carried out by adding the new data to all initial instances and repeating the calculation procedure for the selected K value.\n",
    "* Requires a fairly high amount of data to make realistic predictions and is sensitive to the noise of the analyzed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different ways to calculate the distance in KNN\n",
    "***\n",
    "<center><img src=\"imgs\\distan5.gif\" width=\"80%\" height=\"80%\"></center>\n",
    "<center><img src=\"imgs\\distan6.gif\" width=\"80%\" height=\"80%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building the training set\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the import libraries needed for the simulation:\n",
    "import numpy as np    \n",
    "import tensorflow as tf    \n",
    "import input_data \n",
    "\n",
    "#To construct the data model for the training set, use the input_data.read_data_sets function, introduced earlier:\n",
    "# original code: mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) \n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True) \n",
    "\n",
    "#In our example we will take training phase consisting of 100 MNIST images:\n",
    "train_pixels,train_list_values = mnist.train.next_batch(100) \n",
    "\n",
    "#While we test our algorithm for 10 images:\n",
    "test_pixels,test_list_of_values  = mnist.test.next_batch(10) \n",
    "\n",
    "#Finally, we define the tensors train_pixel_tensor and test_pixel_tensor we use to construct our classifier:\n",
    "train_pixel_tensor = tf.placeholder(\"float\", [None, 784])\n",
    "test_pixel_tensor = tf.placeholder(\"float\", [784])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function and optimization \n",
    "***\n",
    "\n",
    "### The cost function is represented by the distance in terms of pixels:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wrong: distance = tf.reduce_sum(tf.abs(tf.add(train_pixel_tensor, tf.neg(test_pixel_tensor))), reduction_indices=1)\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(train_pixel_tensor, tf.negative(test_pixel_tensor))), 1) # Manhattan\n",
    "# distance = tf.sqrt(tf.reduce_sum(tf.pow(tf.add(train_pixel_tensor, tf.negative(test_pixel_tensor)),2), 1)) # Euclidean\n",
    "\n",
    "test_add = tf.add(train_pixel_tensor, tf.negative(test_pixel_tensor))\n",
    "\n",
    "# Finally, to minimize the distance function, we use arg_min , which returns the index with the smallest distance (nearest neighbor):\n",
    "# wrong: pred = tf.arg_min(distance, 0)\n",
    "pred = tf.argmin(distance, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The tf.reduce function sum computes the sum of elements across the dimensions of a tensor. For example (from the TensorFlow on-line manual):\n",
    "    # 'x' is [[1, 1, 1]    \n",
    "    #         [1, 1, 1]]    \n",
    "    tf.reduce_sum(x) ==> 6    \n",
    "    tf.reduce_sum(x, 0) ==> [2, 2, 2]    \n",
    "    tf.reduce_sum(x, 1) ==> [3, 3]    \n",
    "    tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]    \n",
    "    tf.reduce_sum(x, [0, 1]) ==> 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing and algorithm evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy is a parameter that helps us to compute the final result of the classifier:\n",
    "accuracy = 0 \n",
    "\n",
    "#Initialize the variables:\n",
    "## wrong: init = tf.initialize_all_variables() \n",
    "init = tf.global_variables_initializer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test N°  0 Predicted Class:  7 True Class:  7\n",
      "Test N°  1 Predicted Class:  2 True Class:  2\n",
      "Test N°  2 Predicted Class:  1 True Class:  1\n",
      "Test N°  3 Predicted Class:  0 True Class:  0\n",
      "Test N°  4 Predicted Class:  4 True Class:  4\n",
      "Test N°  5 Predicted Class:  1 True Class:  1\n",
      "Test N°  6 Predicted Class:  4 True Class:  4\n",
      "Test N°  7 Predicted Class:  9 True Class:  9\n",
      "Test N°  8 Predicted Class:  6 True Class:  5\n",
      "Test N°  9 Predicted Class:  9 True Class:  9\n",
      "Result =  0.8999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Start the simulation:\n",
    "with tf.Session() as sess:        \n",
    "    sess.run(init)\n",
    "    for i in range(len(test_list_of_values)): \n",
    "        # Then we evaluate the nearest neighbor index, using the pred function, defined earlier:\n",
    "        nn_index = sess.run(pred, feed_dict={train_pixel_tensor:train_pixels, test_pixel_tensor:test_pixels[i,:]})\n",
    "\n",
    "        # Finally, we find the nearest neighbor class label and compare it to its true label:\n",
    "        print (\"Test N° \", i,\"Predicted Class: \", np.argmax(train_list_values[nn_index]), \"True Class: \", np.argmax(test_list_of_values[i]))\n",
    "\n",
    "        if np.argmax(train_list_values[nn_index]) == np.argmax(test_list_of_values[i]):\n",
    "            #Then we evaluate and report the accuracy of the classifier:\n",
    "            accuracy += 1./len(test_pixels)        \n",
    "\n",
    "    print (\"Result = \", accuracy )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
